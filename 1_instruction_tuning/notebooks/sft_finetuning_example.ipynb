{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with SFTTrainer\n",
    "\n",
    "This notebook demonstrates how to fine-tune the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer` from the `trl` library. The notebook cells run and will finetune the model. You can select your difficulty by trying out different datasets.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Fine-Tuning SmolLM2 with SFTTrainer</h2>\n",
    "    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p> \n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>üê¢ Use the `HuggingFaceTB/smoltalk` dataset</p>\n",
    "    <p>üêï Try out the `bigcode/the-stack-smol` dataset and finetune a code generation model on a specific subset `data/python`.</p>\n",
    "    <p>ü¶Å Select a dataset that relates to a real world use case your interested in</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f403485e0c614b5aad2ebc8ea9aff75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "model.config.eos_token_id = 3 \n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-Haiku\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with the base model\n",
    "\n",
    "Here we will try out the base model which does not have a chat template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "user\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n",
    "\n",
    "**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'generation_model', 'generation_prompt', 'generations', 'scores', 'chosen', 'chosen_score', 'rejected', 'rejected_score', 'tie', 'difference_in_score', 'system'],\n",
       "        num_rows: 3298\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'generation_model', 'generation_prompt', 'generations', 'scores', 'chosen', 'chosen_score', 'rejected', 'rejected_score', 'tie', 'difference_in_score', 'system'],\n",
       "        num_rows: 825\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Write a haiku about the melting glacier and its connection to climate change.',\n",
       " 'generation_model': ['TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ',\n",
       "  'TheBloke/OpenHermes-2.5-Mistral-7B-AWQ'],\n",
       " 'generation_prompt': ['<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  '<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\n'],\n",
       " 'generations': [\"Silent, massive glacier,\\nMelts away with every day,\\nClimate change's toll rises.\",\n",
       "  \"Melting ice reveals,\\nClimate's wrath, no place to hide,\\nMother Earth laments.\",\n",
       "  'Melting glacial ice,\\nA world in disarray, cries -\\nClimate change, the price.',\n",
       "  \"Silent ice crumbles away,\\nEarth's fever burns, it trembles,\\nNature's cry, hear it say.\",\n",
       "  'Melting ice, so slow,\\nA warning sign of change,\\nCry for a new dawn.',\n",
       "  \"Melting glacier cries,\\nWhispering of human error,\\nClimate change's sighs.\",\n",
       "  \"Glistening ice retreats,\\nA slow yet relentless march,\\nClimate's disregard.\",\n",
       "  'Melting glaciers in sight,\\nClimate change, a silent thief,\\nLeaves only memories.',\n",
       "  \"Melting ice, so slow,\\nAs Earth's temperature rises,\\nClimate change, we know.\",\n",
       "  \"Melting ice gives way,\\nThe earth's breath, a gentle sigh,\\nAs climate changes.\",\n",
       "  \"Slowly fading ice\\nMelting glaciers, world's tears\\nClimate change's cost high\",\n",
       "  \"Melting glacier cries,\\nSilent tears of rising heat,\\nEarth's warning, heed it.\",\n",
       "  \"Melting ice, it flows\\nA silent witness to change\\nEarth's temperature rises\",\n",
       "  \"Melting ice flows down,\\nClimate change leaves scars behind,\\nNature's silent cry.\",\n",
       "  \"Glacier slowly bleeds,\\nSilent cries for changing skies,\\nClimate's bitter truth.\",\n",
       "  \"Melting glacier's cry,\\nSilent plea to save our Earth,\\nClimate change's pain revealed.\",\n",
       "  'Melting glacier, \\nCrimson sun sets in haste, \\nEarth cries for help now.',\n",
       "  \"Melting glaciers roar,\\nIce retreats, a climate cry,\\nNature's alarm sounds.\",\n",
       "  \"Melting glacier's cry,\\nIcebergs drifting in the sky,\\nClimate change's lament.\",\n",
       "  \"Melting ice in sight,\\nClimate change, a cruel reign,\\nNature's silent cry.\"],\n",
       " 'scores': [1, 4, 3, 2, 3, 2, 3, 2, 4, 4, 3, 4, 3, 4, 4, 2, 3, 4, 4, 4],\n",
       " 'chosen': \"Melting ice reveals,\\nClimate's wrath, no place to hide,\\nMother Earth laments.\",\n",
       " 'chosen_score': 4,\n",
       " 'rejected': \"Silent, massive glacier,\\nMelts away with every day,\\nClimate change's toll rises.\",\n",
       " 'rejected_score': 1,\n",
       " 'tie': False,\n",
       " 'difference_in_score': 3,\n",
       " 'system': 'You are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(path=\"davanstrien/haiku_dpo\", name=\"default\")\n",
    "split_ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "ds = DatasetDict({\"train\": split_ds[\"train\"], \"test\": split_ds[\"test\"]})\n",
    "\n",
    "display(ds)\n",
    "display(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 3298\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 825\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'messages': \"<|im_start|>system\\nYou are a poet specialising in creating Haiku. \\nYour haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\\nBeyond being technically correct, your haiku should also be beautiful and meaningful<|im_end|>\\n<|im_start|>user\\nWrite a haiku about the melting glacier and its connection to climate change.<|im_end|>\\n<|im_start|>assistant\\nMelting ice reveals,\\nClimate's wrath, no place to hide,\\nMother Earth laments.<|im_end|>\\n\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_sample(sample: Dict) -> Dict:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sample[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "        {\"role\": \"assistant\", \"content\": sample[\"chosen\"]},\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"messages\": tokenizer.decode(\n",
    "            token_ids=tokenizer.apply_chat_template(\n",
    "                conversation=messages, tokenize=True, add_generation_prompt=False\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "ds = ds.map(\n",
    "    process_sample,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    "    desc=\"Processing dataset\",\n",
    ")\n",
    "display(ds)\n",
    "display(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the SFTTrainer\n",
    "\n",
    "The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uziel/Development/smol-course/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9749af377f47b2981fc6c32fc01092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"messages\",  # The field in the dataset containing the text\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=512,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=4,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=8,  # Frequency of logging training metrics\n",
    "    save_steps=128,  # Frequency of saving model checkpoints\n",
    "    eval_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=32,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    "    hub_model_id=finetune_name,  # Set a unique name for your model\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=ds[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='512' max='512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [512/512 02:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>0.784710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.722700</td>\n",
       "      <td>0.710662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.686295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.665852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.654378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.638700</td>\n",
       "      <td>0.644708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.601200</td>\n",
       "      <td>0.634113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>0.625885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>0.620998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.621300</td>\n",
       "      <td>0.614585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.586200</td>\n",
       "      <td>0.609430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.542100</td>\n",
       "      <td>0.608380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.584400</td>\n",
       "      <td>0.603770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.542700</td>\n",
       "      <td>0.602289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.597700</td>\n",
       "      <td>0.599129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.562900</td>\n",
       "      <td>0.598027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Generate with fine-tuned model</h2>\n",
    "    <p>üêï Use the fine-tuned to model generate a response, just like with the base example..</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n",
      "system\n",
      "You are a poet specialising in creating Haiku. \n",
      "Your haiku consist of three lines, with five syllables in the first line, seven in the second, and five in the third.\n",
      "Beyond being technically correct, your haiku should also be beautiful and meaningful\n",
      "user\n",
      "Write a haiku about the blue sky\n",
      "assistant\n",
      "In the blue sky,\n",
      "A gentle breeze whispers peace,\n",
      "Nature's lullaby.\n",
      "Nature's lullaby.\n",
      "Peaceful, serene,\n",
      "Nature's lullaby.\n",
      "Nature's lullaby.\n",
      "Peaceful, serene,\n",
      "Nature's lullaby.\n",
      "Nature's lullaby.\n",
      "Peaceful, serene,\n",
      "Nature's lullaby.\n",
      "Nature's lullaby.\n",
      "Peaceful, serene,\n",
      "Nature's lullaby.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about the blue sky\"\n",
    "\n",
    "# Format with template\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a poet specialising in creating Haiku. \\n\"\n",
    "            \"Your haiku consist of three lines, with five syllables in the first line, \"\n",
    "            \"seven in the second, and five in the third.\\n\"\n",
    "            \"Beyond being technically correct, your haiku should also be \"\n",
    "            \"beautiful and meaningful\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"After training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíê You're done!\n",
    "\n",
    "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
    "\n",
    "- Try this notebook on a harder difficulty\n",
    "- Review a colleagues PR\n",
    "- Improve the course material via an Issue or PR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
